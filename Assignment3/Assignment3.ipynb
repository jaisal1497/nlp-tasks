{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19c247ef"
      },
      "source": [
        "# Assignment 3: Fine-tuning BERT for Classification Tasks (15 Marks)\n",
        "\n",
        "## Due: March 24, 2022\n",
        "\n",
        "Welcome to Assignment 3 of our course on Natural Language Processing. As the name suggests in this assignment you will learn how to fine-tune a pretrained model like BERT on a downstream task to improve much more superior performance compared to the methods discussed so far. Like previous assignments we will continue to work on the SST-2 sentiment dataset as well ask introduce a new task to work on i.e. [Microsfot Research Paraphrase Corpus](https://www.microsoft.com/en-us/download/details.aspx?id=52398). This assignment will also make heavy use of the [Hugging Face's Transformers Library](https://huggingface.co/docs/transformers/index). Don't worry if you are not familiar with the library, we will discuss its usage in detail.\n",
        "\n",
        "Note: Access to a GPU will be crucial for working on this assignment. So do select a GPU runtime in Colab before you start working.\n",
        "\n",
        "Suggested Reading: [Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/pdf/1810.04805.pdf)\n"
      ],
      "id": "19c247ef"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26c077e7",
        "outputId": "50193239-c63c-403e-fa26-ed26cb077a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    sst_data_dir = \"gdrive/MyDrive/PlakshaNLP/Assignment3/data/SST-2\"\n",
        "    mrpc_data_dir = \"gdrive/MyDrive/PlakshaNLP/Assignment3/data/MRPC\"\n",
        "except:\n",
        "    sst_data_dir = \"/datadrive/t-kabir/work/repos/PlakshaNLP/source/Assignment3/data/SST-2\"\n",
        "    mrpc_data_dir = \"/datadrive/t-kabir/work/repos/PlakshaNLP/source/Assignment3/data/MRPC\""
      ],
      "id": "26c077e7"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3242992d",
        "outputId": "283fcc82-59a8-4853-f0d1-4e7b93f18cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 29.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 63.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 69.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 60.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "!pip install transformers\n",
        "!pip install sklearn\n",
        "!pip install tqdm"
      ],
      "id": "3242992d"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9e7b0e64"
      },
      "outputs": [],
      "source": [
        "# We start by importing libraries that we will be making use of in the assignment.\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "import tqdm"
      ],
      "id": "9e7b0e64"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d9b48ab"
      },
      "source": [
        "Similar to last time we will again be working on the Stanford Sentiment Dataset. This time we will also create a validation set by splitting the training data, which we will use for model selection"
      ],
      "id": "3d9b48ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3f82e9d",
        "outputId": "8e28123e-cef9-4af2-c8d1-b7e874b6708e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Training Examples: 66002\n",
            "Number of Validation Examples: 1347\n",
            "Number of Test Examples: 872\n"
          ]
        }
      ],
      "source": [
        "# We can use pandas to load the datasets\n",
        "train_df = pd.read_csv(f\"{sst_data_dir}/train.tsv\", sep = \"\\t\")\n",
        "test_df = pd.read_csv(f\"{sst_data_dir}/dev.tsv\", sep = \"\\t\")\n",
        "\n",
        "# We reserve 2% of the training data for validation\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.02, random_state = 42)\n",
        "\n",
        "print(f\"Number of Training Examples: {len(train_df)}\")\n",
        "print(f\"Number of Validation Examples: {len(val_df)}\")\n",
        "print(f\"Number of Test Examples: {len(test_df)}\")"
      ],
      "id": "b3f82e9d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "adf24a61",
        "outputId": "63b81085-d66e-43e6-f360-b54d6e1b696f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                sentence  label\n",
              "30842  the earnestness of its execution and skill of ...      1\n",
              "11789  while cherish does n't completely survive its ...      0\n",
              "21981                        this sad , compulsive life       0\n",
              "13804                                if i stay positive       1\n",
              "44994  filmmakers david weissman and bill weber benef...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7be0f046-c626-432c-8f98-165c84c2388b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30842</th>\n",
              "      <td>the earnestness of its execution and skill of ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11789</th>\n",
              "      <td>while cherish does n't completely survive its ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21981</th>\n",
              "      <td>this sad , compulsive life</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13804</th>\n",
              "      <td>if i stay positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44994</th>\n",
              "      <td>filmmakers david weissman and bill weber benef...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7be0f046-c626-432c-8f98-165c84c2388b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7be0f046-c626-432c-8f98-165c84c2388b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7be0f046-c626-432c-8f98-165c84c2388b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# View a sample of the dataset\n",
        "train_df.head()"
      ],
      "id": "adf24a61"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99a6b904"
      },
      "source": [
        "## Task 1: Tokenization and Data Preperation"
      ],
      "id": "99a6b904"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0af7f506"
      },
      "source": [
        "As discussed in the lectures, BERT and other pretrained language models use sub-word tokenization i.e. individual words can also be split into constituent subwords to reduce the vocabulary size. The Transformer library provides tokenizer for all the popular language models. Below we demonstrate how to create and use these tokenizers."
      ],
      "id": "0af7f506"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4e8aa458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "8c177ee743654a7f856594eb44f1e252",
            "14fbe477deb8405cbc0077efe9ffb2bd",
            "9faab1f4a97d4491898b85677ca88c1f",
            "32c2d90041374ab39ac5d1c4cdaef842",
            "b0ff83fea7464fe893b09b5a47a59ae4",
            "b07c28d9769044248a75b274fab48b9b",
            "026ea7d1bf0f4c8ba77178192fa7943d",
            "cb3dd0f3f06d497daf36edc485d638ff",
            "17ad1cb339d3406f97c48659e521d3e9",
            "cb43cdd30f3e40ae8228bce7e65dbc35",
            "dde9a52b53ce4a4babb8d08ca1b191d5",
            "c87f7c27b0a643b99ca14f592cca36a3",
            "3a4f2356e8c847aabd777c12aaf7c898",
            "046905ed284549828924f10ddc554afa",
            "71092919c40f4831a70993d06b0ed8e9",
            "73c6c3a44d464e96b88048f979c1f8f3",
            "cee0b0e54a104eeab139022dc1af1ad8",
            "85b6ddda93254dd1af18f8a4f937b753",
            "4ba73991423947c3a24256b03aa21696",
            "aa2e9185e7954413a9063c72944e744b",
            "4dea7ea618c74a4cbf591eb0c13688a5",
            "89f08eb270eb44658e035c1c87961a54"
          ]
        },
        "outputId": "3f2b2291-ebca-4e1c-8e49-2905ba6b7c67"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c177ee743654a7f856594eb44f1e252"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c87f7c27b0a643b99ca14f592cca36a3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Import the BertTokenizer from the library\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load a pre-trained BERT Tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "id": "4e8aa458"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30724776"
      },
      "source": [
        "`BertTokenizer.from_pretrained` is used to load a pre-trained tokenizer. Notice that we provide the argument `\"bert-base-uncased\"` to the method. This refers to the variant of BERT that we want to use. The term \"base\" means we want to use the smaller BERT variant i.e. the one with 12 layers, and \"uncased\" refers to the fact that it treats upper-case and lower-case characters identically. There are 4 variants available for BERT which are:\n",
        "    - `bert-base-uncased`\n",
        "    - `bert-base-cased`\n",
        "    - `bert-large-uncased`\n",
        "    - `bert-large-cased`\n",
        "Now that we have loaded the tokenizer, let's see how to use it."
      ],
      "id": "30724776"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13523cc5"
      },
      "source": [
        "`tokenize` method can be used to split the text into sequence of tokens"
      ],
      "id": "13523cc5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ccbad09",
        "outputId": "3d3ad5d9-8268-4248-99e1-fdf0489f5de2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'high',\n",
              " '-',\n",
              " 'spirited',\n",
              " 'musical',\n",
              " 'that',\n",
              " 'exquisite',\n",
              " '##ly',\n",
              " 'blend',\n",
              " '##s',\n",
              " 'music',\n",
              " ',',\n",
              " 'and',\n",
              " 'high',\n",
              " 'drama',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "bert_tokenizer.tokenize(\"a high-spirited musical that exquisitely blends music , and high drama .\")"
      ],
      "id": "5ccbad09"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daced828"
      },
      "source": [
        "Notice how the tokenizer not only splits the text into words but also subwords like \"exquisitely\" is split into \"exquisite\" and \"ly\". "
      ],
      "id": "daced828"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7260263"
      },
      "source": [
        "Another use case of the tokenizer is to convert the tokens into indices. This is important because BERT and almost all language models takes as the inputs a sequence of token ids, which they use to map into embeddings. `convert_tokens_to_ids` method can be used to do this"
      ],
      "id": "d7260263"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbee421b",
        "outputId": "9881f885-8601-4fef-9bee-5236dd9d20b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1037, 2152, 1011, 24462, 3315, 2008, 19401, 2135, 12586, 2015, 2189, 1010, 1998, 2152, 3689, 1012]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"a high-spirited musical that exquisitely blends music , and high drama .\"\n",
        "tokens = bert_tokenizer.tokenize(sentence)\n",
        "token_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(token_ids)"
      ],
      "id": "dbee421b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efd54a05"
      },
      "source": [
        "The two steps can also be combined by simply calling the tokenizer object"
      ],
      "id": "efd54a05"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfcbb696",
        "outputId": "28ca912c-09ff-468f-c46d-2ce4337f1589"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1037, 2152, 1011, 24462, 3315, 2008, 19401, 2135, 12586, 2015, 2189, 1010, 1998, 2152, 3689, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "bert_tokenizer(sentence)"
      ],
      "id": "bfcbb696"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ab48ddf"
      },
      "source": [
        "Notice that it returns a bunch of things in addition to the ids. The `\"input_ids\"` are just the token ids that we obtained in the previous cell. However you will notice that it has a few additional ids, it starts with 101 and ends with 102. These are what we call special tokens and correspond the \\[CLS\\] and \\[SEP\\] tokens used by BERT. \n",
        "\n",
        "`\"token_type_ids\"` contains which sequence does a particular token belongs to. This is mainly used for sentence pair tasks and can be ignored for now.\n",
        "\n",
        "`\"attention_mask`\" is a mask vector that indicates if a particular token corresponds to padding. Padding is extremely important when we are dealing with variable length sequences, which is almost always the case. Through padding we can ensure that all the sequences in a batch are of same size. However, while processing the sequence we need ignore these padding tokens, hence a mask is required to identify such tokens.\n",
        "\n",
        "Padding can be enabled by providing a value for `max_length` argument and setting `padding=\"max_length\"`, as shown below"
      ],
      "id": "2ab48ddf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d2c0925",
        "outputId": "5587f778-10de-40d3-ecd1-ed12aeee502f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Ids:\n",
            " tensor([[  101,  1037,  2152,  1011, 24462,  3315,  2008, 19401,  2135, 12586,\n",
            "          2015,  2189,  1010,  1998,  2152,  3689,  1012,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]])\n",
            "\n",
            "Attention Mask:\n",
            " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer_output = bert_tokenizer(sentence, max_length=32, padding=\"max_length\", truncation = True, return_tensors=\"pt\")\n",
        "input_ids = tokenizer_output[\"input_ids\"]\n",
        "attn_mask = tokenizer_output[\"attention_mask\"]\n",
        "print(f\"Input Ids:\\n {input_ids}\\n\")\n",
        "\n",
        "print(f\"Attention Mask:\\n {attn_mask}\\n\")"
      ],
      "id": "1d2c0925"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc12c5d0"
      },
      "source": [
        "Notice how 0s get appended to the input ids sequence, and the same is also reflected in the output of `attn_mask` where `0` indicates that the particular token was padded and `1` means otherwise.  `truncation = True` ensures that if a sequence has a length greater than `max_length` it gets truncated. Setting `return_tensors=\"pt\"` results in the outputs as torch tensors"
      ],
      "id": "fc12c5d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4627768e"
      },
      "source": [
        "## Task 1.1: Custom Dataset Class (2 Marks)\n",
        "\n",
        "Now that we know how to use the hugging face tokenizers we can define the custom `torch.utils.Dataset` class like we did in the previous assignments to process and store the data as well as provides a way to iterate through the dataset. Implement the `SST2BertDataset` class below. Recall to create a custom class you need to implement 3 methods `__init__`, `__len__` and `__getitem__`."
      ],
      "id": "4627768e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65ffd2ac"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SST2BertDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, sentences, labels, seq_len, bert_variant = \"bert-base-uncased\"):\n",
        "        \"\"\"\n",
        "        Constructor for the `SST2BertDataset` class. Stores the `sentences` and `labels` which can then be used by\n",
        "        other methods. Also initializes the tokenizer\n",
        "        \n",
        "        Inputs:\n",
        "            - sentences (list) : A list of movie reviews\n",
        "            - labels (list): A list of sentiment labels corresponding to each review\n",
        "            - seq_len (int): Length of the sequence to use.\n",
        "                             If number of tokens are lower than `seq_len` add padding otherwise truncate\n",
        "        \"\"\"\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.seq_len = seq_len\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_variant)\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        \n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset i.e. the number of reviews present in the dataset\n",
        "        \"\"\"\n",
        "        length = len(self.labels)\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        \n",
        "        return length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns the training example corresponding to review present at the `idx` position in the dataset\n",
        "        \n",
        "        Inputs:\n",
        "            - idx (int): Index corresponding to the review,label to be returned\n",
        "            \n",
        "        Returns:\n",
        "            - input_ids (torch.tensor): Indices of the tokens in the sentence at `idx` position.\n",
        "                                        Shape of the tensor should be (`seq_len`,)\n",
        "            - mask (torch.tensor): Attention mask indicating which tokens are padded.\n",
        "                                   Shape of the tensor should be (`seq_len`,)\n",
        "            - label (int): Sentiment label for the corresponding sentence\n",
        "        \n",
        "        Hint: To get the output from the tokenizer in the form of torch tensors set return_tensors=\"pt\" when calling self.tokenizer \n",
        "        \"\"\"\n",
        "        tokenizer_output = self.tokenizer(self.sentences[idx], max_length=self.seq_len, padding=\"max_length\", truncation = True, return_tensors=\"pt\")\n",
        "        input_ids = tokenizer_output[\"input_ids\"]\n",
        "        mask = tokenizer_output[\"attention_mask\"]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        \n",
        "        return input_ids.squeeze(0), mask.squeeze(0), label"
      ],
      "id": "65ffd2ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b8c8dea",
        "outputId": "9e2804cf-7dc8-4d0f-e2d7-dbe6059aa294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Cases\n",
            "Sample Test Case 1: Checking if `__len__` is implemented correctly\n",
            "Dataset Length: 3\n",
            "Expected Length: 3\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\n",
            "input_ids:\n",
            " tensor([  101,  4895, 10258,  2378,  8450,  2135, 21657,  1998,  7143,   102,\n",
            "            0,     0])\n",
            "Expected input_ids:\n",
            " tensor([  101,  4895, 10258,  2378,  8450,  2135, 21657,  1998,  7143,   102,\n",
            "            0,     0])\n",
            "mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
            "Expected mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
            "label:\n",
            " 0\n",
            "Expected label:\n",
            " 0\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\n",
            "input_ids:\n",
            " tensor([ 101, 2009, 1005, 1055, 4030, 1011, 1011, 2200, 1010, 2200, 4030,  102])\n",
            "Expected input_ids:\n",
            " tensor([ 101, 2009, 1005, 1055, 4030, 1011, 1011, 2200, 1010, 2200, 4030,  102])\n",
            "mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "Expected mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "label:\n",
            " 0\n",
            "Expected label:\n",
            " 0\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 2`\n",
            "input_ids:\n",
            " tensor([  101,  2009,  1005,  1055,  1037, 11951,  1998,  2411, 12473,  4990,\n",
            "         1012,   102])\n",
            "Expected input_ids:\n",
            " tensor([  101,  2009,  1005,  1055,  1037, 11951,  1998,  2411, 12473,  4990,\n",
            "         1012,   102])\n",
            "mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "Expected mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "label:\n",
            " 1\n",
            "Expected label:\n",
            " 1\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Running Sample Test Cases\")\n",
        "\n",
        "sample_sentences = [\"unflinchingly bleak and desperate\",\n",
        "                    \"it 's slow -- very , very slow .\",\n",
        "                    \"it 's a charming and often affecting journey .\"]\n",
        "sample_labels = [0, 0, 1]\n",
        "sample_seq_len = 12\n",
        "sample_dataset = SST2BertDataset(sample_sentences, sample_labels, sample_seq_len)\n",
        "\n",
        "print(f\"Sample Test Case 1: Checking if `__len__` is implemented correctly\")\n",
        "dataset_len= len(sample_dataset)\n",
        "expected_len = len(sample_labels)\n",
        "print(f\"Dataset Length: {dataset_len}\")\n",
        "print(f\"Expected Length: {expected_len}\")\n",
        "assert len(sample_dataset) == len(sample_sentences)\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "print(f\"Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\")\n",
        "sample_idx = 0\n",
        "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_input_ids = torch.tensor([101, 4895, 10258, 2378, 8450, 2135, 21657, 1998, 7143, 102, 0, 0])\n",
        "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
        "expected_label = 0\n",
        "print(f\"input_ids:\\n {input_ids}\")\n",
        "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
        "assert (expected_input_ids == input_ids).all()\n",
        "\n",
        "print(f\"mask:\\n {mask}\")\n",
        "print(f\"Expected mask:\\n {expected_mask}\")\n",
        "assert (expected_mask == mask).all()\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "print(f\"Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\")\n",
        "sample_idx = 1\n",
        "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_input_ids = torch.tensor([101, 2009, 1005, 1055, 4030, 1011, 1011, 2200, 1010, 2200, 4030, 102])\n",
        "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
        "expected_label = 0\n",
        "print(f\"input_ids:\\n {input_ids}\")\n",
        "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
        "assert (expected_input_ids == input_ids).all()\n",
        "\n",
        "print(f\"mask:\\n {mask}\")\n",
        "print(f\"Expected mask:\\n {expected_mask}\")\n",
        "assert (expected_mask == mask).all()\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "print(f\"Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 2`\")\n",
        "sample_idx = 2\n",
        "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_input_ids = torch.tensor([101, 2009, 1005, 1055, 1037, 11951, 1998, 2411, 12473, 4990, 1012, 102])\n",
        "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
        "expected_label = 1\n",
        "print(f\"input_ids:\\n {input_ids}\")\n",
        "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
        "assert (expected_input_ids == input_ids).all()\n",
        "\n",
        "print(f\"mask:\\n {mask}\")\n",
        "print(f\"Expected mask:\\n {expected_mask}\")\n",
        "assert (expected_mask == mask).all()\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n"
      ],
      "id": "7b8c8dea"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ec2e35e"
      },
      "source": [
        "Creating Datasets and Dataloaders for train, validation and test data. Since pretrained models like BERT have millions of parameters, it is common to use a smaller batch size to reduce the memory footprint."
      ],
      "id": "8ec2e35e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3994ab0"
      },
      "outputs": [],
      "source": [
        "seq_len = 128\n",
        "batch_size = 16\n",
        "\n",
        "train_sentences, train_labels = train_df[\"sentence\"].values, train_df[\"label\"].values\n",
        "val_sentences, val_labels = val_df[\"sentence\"].values, val_df[\"label\"].values\n",
        "test_sentences, test_labels = test_df[\"sentence\"].values, test_df[\"label\"].values\n",
        "\n",
        "train_dataset = SST2BertDataset(train_sentences, train_labels, seq_len=seq_len)\n",
        "val_dataset = SST2BertDataset(val_sentences, val_labels, seq_len=seq_len)\n",
        "test_dataset = SST2BertDataset(test_sentences, test_labels, seq_len=seq_len)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "id": "d3994ab0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "849edfc0"
      },
      "source": [
        "## Task 2: Implementing and Training BERT-based Classifier\n",
        "\n",
        "Similar to pretrained tokenizers, the transformers library also provide numerous pre-trained language models that can be fine-tuned on a wide variety of downstream tasks. We demonstrate usage of these models below."
      ],
      "id": "849edfc0"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0536691395494e01a52e4714bcb61dbc",
            "d0a3e59425744c45ae87bde298bfb7fa",
            "e4b9775c37ab401cbcb24dbc44f2ff48",
            "07c9da16915b4727ab981ecf076941b2",
            "5bd4089f2ca8488a86f9b54a3359992d",
            "aecbe9da0699413e98fac8a668751d74",
            "d1b2b311866a40e0ab57634fc85301f2",
            "23309253219648bb8ff941fba0db3f8e",
            "97f13130431c46a2840d15ae3be041c9",
            "6637a2d35ed84a6e8217ff4c5c574f0f",
            "2e445a0c08214f2f91b99bfa19d0f8b7",
            "930269b3fa1f468791afcf0f03b2d383",
            "95e7cc5fb8d241fbb6d74f78c7cfd77c",
            "66b13032a2764b3e9ac225677c757610",
            "44f3810657174c46a7b330a867ffecf1",
            "5fc04757b46f4f41abb5553b6149fa5a",
            "37c647f3384c435892b9bda4cdbcdf24",
            "7b21d599b26d41239ae776badabc5dbd",
            "488cfe0abc7f49188acc43a4f9c616e1",
            "a76ac3e54f8b4de3bf1e287b51a79d85",
            "ac8bdbf63eef4296be62f8ff43df1082",
            "cebddec4b7944291b34eeef3fa2b1bc1"
          ]
        },
        "id": "c9322600",
        "outputId": "1bf9d250-97ec-4400-a51a-cb4ebd3baaf8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0536691395494e01a52e4714bcb61dbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "930269b3fa1f468791afcf0f03b2d383"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Import BertModel from the library\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create an instance of pretrained BERT\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model"
      ],
      "id": "c9322600"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a7f074b"
      },
      "source": [
        "As you can see very similar to how we created pre-trained tokenizer, we can load a pretrained BERT model by calling `BertModel.from_pretrained(bert-base-uncased)`. This can actually be considered just a Pytorch `nn.Module` like `nn.Linear` and can be similarly plugged into a network architecture. Also, notice the model contains 12 BERT layers, where each layer consists of a Self Attention layer followed by a sequence of linear layers and activation functions (MLP), as we discussed when talking about Transformer architecture in the lecture."
      ],
      "id": "4a7f074b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c994b139",
        "outputId": "a9c56b96-0dac-4468-8faa-423aeb23c5d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "sentence = \"a high-spirited musical that exquisitely blends music , and high drama .\"\n",
        "tokenizer_output = bert_tokenizer(sentence, return_tensors=\"pt\")\n",
        "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
        "\n",
        "output = bert_model(input_ids, attention_mask = attn_mask)\n",
        "type(output)"
      ],
      "id": "c994b139"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "919f7bfc"
      },
      "source": [
        "As you can calling `bert_model` returns a bunch of different things. Let's go through them one by one and understand"
      ],
      "id": "919f7bfc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f337132",
        "outputId": "3a4290ec-2268-43f2-b90a-e1870cf4f418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids shape: torch.Size([1, 18])\n",
            "last_hidden_state shape: torch.Size([1, 18, 768])\n"
          ]
        }
      ],
      "source": [
        "last_hidden_state = output.last_hidden_state\n",
        "print(f\"input_ids shape: {input_ids.shape}\")\n",
        "print(f\"last_hidden_state shape: {last_hidden_state.shape}\")"
      ],
      "id": "1f337132"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fa9e5c2"
      },
      "source": [
        "For an input of shape `[1,18]` which just means a single sequence of 18 tokens, last_hidden_state is a tensor of shape `[1, 18, 768]` denoting the contextual embedding of each of the 18 tokens in the sequence. These representations can be then used for solving a downstream task, by adding a linear layer or MLP layer on top. These can be useful for sequence labelling type of tasks."
      ],
      "id": "7fa9e5c2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9c00632",
        "outputId": "d2ee039f-9c1e-41ab-eaea-c74723773299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids shape: torch.Size([1, 18])\n",
            "pooler_output shape: torch.Size([1, 768])\n"
          ]
        }
      ],
      "source": [
        "pooler_output = output.pooler_output\n",
        "print(f\"input_ids shape: {input_ids.shape}\")\n",
        "print(f\"pooler_output shape: {pooler_output.shape}\")"
      ],
      "id": "c9c00632"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88dae74d"
      },
      "source": [
        "`pooler_output` is an aggregate representation of the entire sentence and can be thought of as a sentence embedding. It is obtained by passing the representation of the \\[CLS\\] token through a linear layer. This can be useful for sentence-level tasks like sentiment analysis etc."
      ],
      "id": "88dae74d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5785c1f7"
      },
      "source": [
        "Apart from these two we can also obtain other values by providing additional arguments. Like if we want to obtain attention maps which can be useful for interpretating the model's behavior, we can just specify `output_attentions=True` while calling the model"
      ],
      "id": "5785c1f7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50ae6ee7",
        "outputId": "da409b1f-2938-4948-fb80-3931b9640d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data type of attentions output: <class 'tuple'>\n",
            "Number of elements: 12\n",
            "Shape of individual element: torch.Size([1, 12, 18, 18])\n",
            "Example attention map: tensor([[0.0394, 0.1036, 0.0293, 0.0427, 0.0286, 0.0309, 0.0662, 0.0268, 0.0501,\n",
            "         0.0200, 0.0675, 0.0327, 0.0375, 0.0721, 0.0347, 0.0549, 0.0806, 0.1825],\n",
            "        [0.0649, 0.0452, 0.0417, 0.0654, 0.0789, 0.0430, 0.0496, 0.0633, 0.0430,\n",
            "         0.0539, 0.0473, 0.0497, 0.0574, 0.0621, 0.0497, 0.0575, 0.0777, 0.0497],\n",
            "        [0.0283, 0.0353, 0.0166, 0.0490, 0.0736, 0.0902, 0.0363, 0.1293, 0.0442,\n",
            "         0.0875, 0.0423, 0.0785, 0.0449, 0.0196, 0.0160, 0.1193, 0.0377, 0.0513],\n",
            "        [0.0383, 0.0532, 0.0579, 0.0280, 0.0793, 0.0480, 0.0412, 0.0809, 0.0484,\n",
            "         0.0647, 0.0456, 0.0459, 0.0744, 0.0305, 0.0602, 0.0957, 0.0593, 0.0482],\n",
            "        [0.0096, 0.0322, 0.0157, 0.1455, 0.0169, 0.0352, 0.0554, 0.0405, 0.0580,\n",
            "         0.0260, 0.0883, 0.0265, 0.0851, 0.0867, 0.0172, 0.0415, 0.1230, 0.0967],\n",
            "        [0.0380, 0.0107, 0.0485, 0.0633, 0.0868, 0.0398, 0.0266, 0.1159, 0.0366,\n",
            "         0.0729, 0.0251, 0.0916, 0.0397, 0.0379, 0.0442, 0.1373, 0.0472, 0.0381],\n",
            "        [0.0801, 0.0344, 0.0325, 0.1073, 0.0568, 0.0236, 0.0723, 0.0337, 0.0640,\n",
            "         0.0458, 0.0473, 0.0296, 0.1027, 0.0728, 0.0332, 0.0410, 0.0734, 0.0494],\n",
            "        [0.0415, 0.0189, 0.0266, 0.0727, 0.0583, 0.0658, 0.0531, 0.0565, 0.0794,\n",
            "         0.0456, 0.0387, 0.0877, 0.0609, 0.0534, 0.0245, 0.0904, 0.0805, 0.0456],\n",
            "        [0.0319, 0.0278, 0.0403, 0.0530, 0.1001, 0.0538, 0.0355, 0.1005, 0.0336,\n",
            "         0.0722, 0.0341, 0.0549, 0.0253, 0.0346, 0.0416, 0.1278, 0.0610, 0.0719],\n",
            "        [0.0172, 0.0320, 0.0322, 0.0625, 0.1101, 0.0817, 0.0693, 0.1197, 0.0246,\n",
            "         0.0429, 0.0355, 0.1023, 0.0409, 0.0226, 0.0251, 0.0939, 0.0523, 0.0350],\n",
            "        [0.0363, 0.0226, 0.0340, 0.0575, 0.1067, 0.0600, 0.0259, 0.1548, 0.0492,\n",
            "         0.0905, 0.0263, 0.0482, 0.0357, 0.0145, 0.0287, 0.0976, 0.0395, 0.0719],\n",
            "        [0.0260, 0.0262, 0.0313, 0.0505, 0.1031, 0.0783, 0.0395, 0.1045, 0.0416,\n",
            "         0.0837, 0.0280, 0.0864, 0.0319, 0.0191, 0.0283, 0.1520, 0.0286, 0.0412],\n",
            "        [0.0513, 0.0498, 0.0497, 0.0606, 0.0797, 0.0454, 0.0553, 0.0602, 0.0443,\n",
            "         0.0522, 0.0445, 0.0397, 0.0384, 0.0798, 0.0483, 0.0664, 0.0814, 0.0531],\n",
            "        [0.0498, 0.0470, 0.0322, 0.0880, 0.0663, 0.0355, 0.0730, 0.0612, 0.0563,\n",
            "         0.0505, 0.0467, 0.0299, 0.0831, 0.0671, 0.0312, 0.0640, 0.0677, 0.0506],\n",
            "        [0.0255, 0.0390, 0.0185, 0.0565, 0.0739, 0.0990, 0.0377, 0.1325, 0.0502,\n",
            "         0.0731, 0.0410, 0.0748, 0.0358, 0.0204, 0.0150, 0.1256, 0.0453, 0.0362],\n",
            "        [0.0469, 0.0156, 0.0497, 0.1177, 0.0918, 0.0499, 0.0284, 0.0719, 0.0734,\n",
            "         0.0642, 0.0288, 0.0638, 0.0355, 0.0539, 0.0430, 0.0898, 0.0455, 0.0304],\n",
            "        [0.0401, 0.0677, 0.0507, 0.0809, 0.0627, 0.0355, 0.0606, 0.0488, 0.0535,\n",
            "         0.0381, 0.0492, 0.0317, 0.0692, 0.0741, 0.0474, 0.0567, 0.0848, 0.0484],\n",
            "        [0.0384, 0.0791, 0.0282, 0.0507, 0.0405, 0.0425, 0.0704, 0.0499, 0.0708,\n",
            "         0.0335, 0.0647, 0.0371, 0.0761, 0.0610, 0.0337, 0.0591, 0.0995, 0.0646]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "output = bert_model(input_ids, attention_mask = attn_mask, output_attentions=True)\n",
        "attentions = output.attentions\n",
        "print(f\"Data type of attentions output: {type(attentions)}\")\n",
        "print(f\"Number of elements: {len(attentions)}\")\n",
        "print(f\"Shape of individual element: {attentions[0].shape}\")\n",
        "print(f\"Example attention map: {attentions[0][0,0]}\")"
      ],
      "id": "50ae6ee7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3a4519b"
      },
      "source": [
        "As you can see `attentions` is a tuple containing 12 elements which corresponds to the attention maps of each of the 12 layers in the network. Further each layer's attention maps also contains 12 attention maps corresponding to 12 heads in each layer. A single attention map as you can see is a 18x18 matrix representing the attention pattern for all the tokens in the sequence"
      ],
      "id": "a3a4519b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a79f03f"
      },
      "source": [
        "### Task 2.1: Implementing BERT-based Classifier (2 Marks)\n",
        "\n",
        "In this task you will implement a bert-based classifier in Pytorch very similar to how we created bag of word classifiers in the previous assignments. Instead of using `nn.Linear` here we will simply use `BertModel` as a component. Implement the `BertClassiferModel` module below with the architecture BertModel->Linear->Sigmoid"
      ],
      "id": "8a79f03f"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0f626042"
      },
      "outputs": [],
      "source": [
        "class BertClassifierModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_hidden = 768, bert_variant = \"bert-base-uncased\"):\n",
        "        \"\"\"\n",
        "        Define the architecture of Bert-Based classifier.\n",
        "        You will mainly need to define 3 components, first a BERT layer\n",
        "        using `BertModel` from transformers library,\n",
        "        a linear layer to map the representation from Bert to the output,\n",
        "        and a sigmoid layer to map the score to a proability\n",
        "        \n",
        "        Inputs:\n",
        "            - d_hidden (int): Size of the hidden representations of bert\n",
        "            - bert_variant (str): BERT variant to use\n",
        "        \"\"\"\n",
        "        super(BertClassifierModel, self).__init__()\n",
        "        self.bert_layer = BertModel.from_pretrained(bert_variant)\n",
        "        self.output_layer = nn.Linear(d_hidden,1)\n",
        "        self.sigmoid_layer = nn.Sigmoid()\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        \n",
        "    def forward(self, input_ids, attn_mask):\n",
        "        \"\"\"\n",
        "        Forward Passes the inputs through the network and obtains the prediction\n",
        "        \n",
        "        Inputs:\n",
        "            - input_ids (torch.tensor): A torch tensor of shape [batch_size, seq_len]\n",
        "                                        representing the sequence of token ids\n",
        "            - attn_mask (torch.tensor): A torch tensor of shape [batch_size, seq_len]\n",
        "                                        representing the attention mask such that padded tokens are 0 and rest 1\n",
        "                                        \n",
        "        Returns:\n",
        "          - output (torch.tensor): A torch tensor of shape [batch_size,] obtained after passing the input to the network\n",
        "                                        \n",
        "        \n",
        "        Hint: Recall which of the outputs from BertModel is appropriate for the sentence classification task.\n",
        "        \"\"\"\n",
        "        \n",
        "        output = self.bert_layer(input_ids, attention_mask = attn_mask)\n",
        "        output = self.output_layer(output.pooler_output)\n",
        "        #print(output.shape)\n",
        "        output = self.sigmoid_layer(output)\n",
        "        #print(output)\n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        \n",
        "        return output.squeeze(-1) # Question: Why do squeeze() here? "
      ],
      "id": "0f626042"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c65d7c7",
        "outputId": "8dbbf882-5bb2-4444-b793-1ff17eb264cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Cases!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Test Case 1\n",
            "Input Sentence: a high-spirited musical that exquisitely blends music , and high drama .\n",
            "Model Output: [0.43614867]\n",
            "Expected Output: [0.43614867]\n",
            "Test Case Passed! :)\n",
            "******************************\n",
            "\n",
            "Sample Test Case 2 (Checking how padding effects the output. It shouldn't!)\n",
            "Input Sentence: a high-spirited musical that exquisitely blends music , and high drama .\n",
            "Model Output: [0.43614867]\n",
            "Expected Output: [0.43614867]\n",
            "Test Case Passed! :)\n",
            "******************************\n",
            "\n",
            "Sample Test Case 3. Checking if the model works for batched inputs\n",
            "Input Sentences: ['a high-spirited musical that exquisitely blends music , and high drama .', 'unflinchingly bleak and desperate']\n",
            "Model Output: [0.43614867 0.46988726]\n",
            "Expected Output: [0.43614867 0.46988717]\n",
            "Test Case Passed! :)\n",
            "******************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Running Sample Test Cases!\")\n",
        "torch.manual_seed(42)\n",
        "model = BertClassifierModel()\n",
        "\n",
        "print(\"Sample Test Case 1\")\n",
        "sentence = \"a high-spirited musical that exquisitely blends music , and high drama .\"\n",
        "tokenizer_output = bert_tokenizer(sentence, return_tensors=\"pt\")\n",
        "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
        "bert_cls_out = model(input_ids, attn_mask).detach().numpy()\n",
        "expected_bert_cls_out = np.array([0.43614867])\n",
        "print(f\"Input Sentence: {sentence}\")\n",
        "print(f\"Model Output: {bert_cls_out}\")\n",
        "print(f\"Expected Output: {expected_bert_cls_out}\")\n",
        "\n",
        "assert bert_cls_out.shape == expected_bert_cls_out.shape\n",
        "assert np.allclose(bert_cls_out, expected_bert_cls_out, 1e-4)\n",
        "print(\"Test Case Passed! :)\")\n",
        "print(\"******************************\\n\")\n",
        "\n",
        "print(\"Sample Test Case 2 (Checking how padding effects the output. It shouldn't!)\")\n",
        "sentence = \"a high-spirited musical that exquisitely blends music , and high drama .\"\n",
        "tokenizer_output = bert_tokenizer(sentence,max_length = 30, padding = \"max_length\", return_tensors=\"pt\")\n",
        "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
        "bert_cls_out = model(input_ids, attn_mask).detach().numpy()\n",
        "expected_bert_cls_out = np.array([0.43614867])\n",
        "print(f\"Input Sentence: {sentence}\")\n",
        "print(f\"Model Output: {bert_cls_out}\")\n",
        "print(f\"Expected Output: {expected_bert_cls_out}\")\n",
        "\n",
        "assert bert_cls_out.shape == expected_bert_cls_out.shape\n",
        "assert np.allclose(bert_cls_out, expected_bert_cls_out, 1e-4)\n",
        "print(\"Test Case Passed! :)\")\n",
        "print(\"******************************\\n\")\n",
        "\n",
        "print(\"Sample Test Case 3. Checking if the model works for batched inputs\")\n",
        "sentences = [\n",
        "    \"a high-spirited musical that exquisitely blends music , and high drama .\",\n",
        "    \"unflinchingly bleak and desperate\"\n",
        "]\n",
        "tokenizer_output = bert_tokenizer(sentences,max_length = 30, padding = \"max_length\", return_tensors=\"pt\")\n",
        "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
        "bert_cls_out = model(input_ids, attn_mask).detach().numpy()\n",
        "expected_bert_cls_out = np.array([0.43614867, 0.46988717])\n",
        "print(f\"Input Sentences: {sentences}\")\n",
        "print(f\"Model Output: {bert_cls_out}\")\n",
        "print(f\"Expected Output: {expected_bert_cls_out}\")\n",
        "\n",
        "assert bert_cls_out.shape == expected_bert_cls_out.shape\n",
        "assert np.allclose(bert_cls_out, expected_bert_cls_out, 1e-4)\n",
        "print(\"Test Case Passed! :)\")\n",
        "print(\"******************************\\n\")\n"
      ],
      "id": "1c65d7c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f38f523b"
      },
      "source": [
        "### Task 2.2: Training and Evaluating the Model (5 Marks)\n",
        "\n",
        "Now that we have implemented the custom Dataset and a BERT based classifier model, we can start training and evaluating the model. This time we will modify the training loop slightly. At the end of each training epoch we will now evaluate on the validation data and check the accuracy. Based on this we will select the best model across the epochs that obtains highest validation accuracy. You will need to implement the `train` and `evaluate` functions below."
      ],
      "id": "f38f523b"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "selL4JArcXf_"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(pred_labels,act_labels):\n",
        "\n",
        "  accuracy = None\n",
        "  # YOUR CODE HERE\n",
        "  pred_labels=convert_probs_to_labels(pred_labels)\n",
        "  accuracy = np.sum(np.equal(act_labels, pred_labels)) / len(act_labels)\n",
        "  #raise NotImplementedError()\n",
        "\n",
        "  return accuracy\n",
        "def convert_probs_to_labels(probs, threshold = 0.5):\n",
        "  \"\"\"\n",
        "  Convert the probabilities to labels by using the specified threshold\n",
        "\n",
        "  Inputs:\n",
        "    - probs (numpy.ndarray): A numpy 1d array containing the probabilities predicted by the classifier model\n",
        "    - threshold (float): A threshold value beyond which we assign a positive label i.e 1 and 0 below it\n",
        "\n",
        "  Returns:\n",
        "    - labels (numpy.ndarray): Labels obtained after thresholding\n",
        "    \n",
        "  \"\"\"\n",
        "    \n",
        "  labels = None\n",
        "  labels=(probs>threshold).astype(int)\n",
        "  # YOUR CODE HERE\n",
        "  #raise NotImplementedError()\n",
        "\n",
        "  return labels"
      ],
      "id": "selL4JArcXf_"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "af1e43b1"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_dataloader, threshold = 0.5, device = \"cpu\"):\n",
        "\n",
        "    \"\"\"\n",
        "    Evaluates `model` on test dataset\n",
        "\n",
        "    Inputs:\n",
        "        - model (BertClassifierModel): Logistic Regression model to be evaluated\n",
        "        - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n",
        "        - threshold (float): Probability Threshold above which we consider label as 1 and 0 below\n",
        "\n",
        "    Returns:\n",
        "        - accuracy (float): Average accuracy over the test dataset \n",
        "    \"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    accuracy = 0\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "      # by specifying `torch.no_grad`, it ensures no gradients are calcuated while running the model,\n",
        "  # this makes the computation much more faster\n",
        "    with torch.no_grad():\n",
        "      for test_batch in test_dataloader:\n",
        "        features, mask,labels = test_batch\n",
        "        features = features.float().to(device).long()\n",
        "        labels = labels.float().to(device).long()\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        # Step 1: Get probability predictions from the model and store it in `pred_probs`\n",
        "        # YOUR CODE HERE\n",
        "        pred_probs = model(features,mask)\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "        # Convert predictions and labels to numpy arrays from torch tensors as they are easier to operate for computing metrics\n",
        "        pred_probs = pred_probs.detach().cpu().numpy()\n",
        "        labels = labels.detach().cpu().numpy()\n",
        "\n",
        "        # Step 2: Get accuracy of predictions and store it in `batch_accuracy`\n",
        "        batch_accuracy = None\n",
        "      \n",
        "        batch_accuracy=get_accuracy(pred_probs,labels)\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "        accuracy += batch_accuracy\n",
        "\n",
        "      # Divide by number of batches to get average accuracy\n",
        "      accuracy = accuracy / len(test_dataloader)\n",
        "    #raise NotImplementedError()\n",
        "    \n",
        "    return accuracy\n",
        "    \n",
        "\n",
        "def train(model, train_dataloader, val_dataloader,\n",
        "          lr = 1e-5, num_epochs = 3,\n",
        "          device = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Runs the training loop. Define the loss function as BCELoss like the last tine\n",
        "    and optimizer as Adam and traine for `num_epochs` epochs.\n",
        "\n",
        "    Inputs:\n",
        "        - model (BertClassifierModel): BERT based classifer model to be trained\n",
        "        - train_dataloader (torch.utils.DataLoader): A dataloader defined over the training dataset\n",
        "        - val_dataloader (torch.utils.DataLoader): A dataloader defined over the validation dataset\n",
        "        - lr (float): The learning rate for the optimizer\n",
        "        - num_epochs (int): Number of epochs to train the model for.\n",
        "        - device (str): Device to train the model on. Can be either 'cuda' (for using gpu) or 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        - best_model (BertClassifierModel): model corresponding to the highest validation accuracy (checked at the end of each epoch)\n",
        "        - best_val_accuracy (float): Validation accuracy corresponding to the best epoch\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "    model = model.to(device)\n",
        "    \n",
        "    best_val_accuracy = float(\"-inf\")\n",
        "    best_model = None\n",
        "    \n",
        "    # 1. Define Loss function and optimizer\n",
        "    loss_fn = None\n",
        "    optimizer = None\n",
        "    loss_fn = nn.BCELoss()\n",
        "    optimizer = Adam(model.parameters(), lr)\n",
        "    # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Since we are evaluating model at the end of every epoch, it is important to bring it back to train mode\n",
        "        epoch_loss = 0\n",
        "        print(epoch)\n",
        "        # 2. Write Training Loop (store the loss for each batch in epoch_loss like done in previous assignments)\n",
        "        # YOUR CODE HERE\n",
        "        for train_batch in tqdm.tqdm(train_dataloader):\n",
        "          #print(\"inside train batch\")\n",
        "        #raise NotImplementedError()\n",
        "          optimizer.zero_grad()\n",
        "          #print(train_batch)\n",
        "          features, mask,labels = train_batch\n",
        "\n",
        "      # Most nn modules and loss functions assume the inputs are of type Float, so convert both features and labels to floats\n",
        "          features = features.float()\n",
        "          labels = labels.float()\n",
        "          # Transfer the features and labels to device\n",
        "          features = features.to(device).long()\n",
        "          labels = labels.to(device).long()\n",
        "          mask=mask.to(device).long()\n",
        "\n",
        "\n",
        "          # Step 3: Feed the input features to the model to get predictions\n",
        "          #preds = None\n",
        "          preds = model(features,mask)\n",
        "          # YOUR CODE HERE\n",
        "          #raise NotImplementedError()\n",
        "\n",
        "          # Step 4: Compute the loss and perform backward pass\n",
        "          loss = None\n",
        "          # YOUR CODE HERE\n",
        "          lbl=labels.float()\n",
        "          loss = loss_fn(preds, lbl)\n",
        "          #print(\"implemented loss\")\n",
        "          loss.backward()\n",
        "          #raise NotImplementedError()\n",
        "      \n",
        "          # Step 5: Take optimizer step\n",
        "          # YOUR CODE HERE\n",
        "          #print(\"before optimizer\")\n",
        "          optimizer.step()\n",
        "          #raise NotImplementedError()\n",
        "\n",
        "          # Store loss value for tracking\n",
        "          epoch_loss += loss.item()\n",
        "          #print(\"after epoch_loss\")\n",
        "        #print(\"before epoch_loss final\")\n",
        "        epoch_loss = epoch_loss / len(train_dataloader)\n",
        "        \n",
        "        # 3. Evaluate on validation data by calling `evaluate` and store the validation accuracy in `val_accurracy`\n",
        "        val_accuracy = 0\n",
        "        # YOUR CODE HERE\n",
        "        #print(\"before evaluate\")\n",
        "        val_accuracy=evaluate(model,train_dataloader,0.5,\"cuda\")\n",
        "        #raise NotImplementedError()\n",
        "        #print(\"after val accuracy\")\n",
        "        # Model selection\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_model = copy.deepcopy(model) # Create a copy of model\n",
        "        \n",
        "        print(f\"Epoch {epoch} completed | Average Training Loss: {epoch_loss} | Validation Accuracy: {val_accuracy}\")\n",
        " \n",
        "    return best_model, best_val_accuracy"
      ],
      "id": "af1e43b1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b408695c",
        "outputId": "0edae9ea-04d8-4c7c-dfeb-fa878112fc3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 100 data points for sanity check\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 20.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 completed | Average Training Loss: 0.6932587170600891 | Validation Accuracy: 0.75\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 21.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed | Average Training Loss: 0.6349138021469116 | Validation Accuracy: 0.9\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 20.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed | Average Training Loss: 0.4696041166782379 | Validation Accuracy: 0.99\n",
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 20.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 completed | Average Training Loss: 0.34586642384529115 | Validation Accuracy: 1.0\n",
            "4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 20.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 completed | Average Training Loss: 0.1976077139377594 | Validation Accuracy: 1.0\n",
            "Best Validation Accuracy: 1.0\n",
            "Expected Best Validation Accuracy: 0.99\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "print(\"Training on 100 data points for sanity check\")\n",
        "sample_sentences = train_df[\"sentence\"].values.tolist()[:100]\n",
        "sample_labels = train_df[\"label\"].values.tolist()[:100]\n",
        "sample_dataset = SST2BertDataset(sample_sentences, sample_labels, seq_len=32)\n",
        "sample_dataloader = DataLoader(sample_dataset, batch_size=4)\n",
        "\n",
        "model = BertClassifierModel()\n",
        "best_model, best_val_acc = train(model, sample_dataloader, sample_dataloader, num_epochs = 5, device = \"cuda\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc}\")\n",
        "print(f\"Expected Best Validation Accuracy: {0.99}\")"
      ],
      "id": "b408695c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e958b3c7"
      },
      "source": [
        " You can expect the validation accuracy of 0.99 by the end of training. This is so high because we trained on just 100 examples and just use those for validation for a sanity check. This is often done to debug the model and training loop. Let's now train on the entire dataset. This can take some time approximately 50 minutes per epoch, since we are fine-tuning all the 12 layers of BERT."
      ],
      "id": "e958b3c7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "298b6cf0",
        "outputId": "b906bf30-cf4f-472e-da1d-66897a155a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4126/4126 [14:59<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 completed | Average Training Loss: 0.20951996469349699 | Validation Accuracy: 0.9711888027144935\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4126/4126 [14:58<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed | Average Training Loss: 0.11043857011126496 | Validation Accuracy: 0.9815347794474066\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4126/4126 [14:58<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed | Average Training Loss: 0.07470167261921681 | Validation Accuracy: 0.9877302472127969\n"
          ]
        }
      ],
      "source": [
        "model = BertClassifierModel()\n",
        "best_model, best_val_acc = train(model, train_loader, val_loader, num_epochs = 3, device = \"cuda\")"
      ],
      "id": "298b6cf0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a397015f"
      },
      "source": [
        "You should expect about ~95% validation accuracy. Let's now check how does this model performs on the test data"
      ],
      "id": "a397015f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6a418dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97da23eb-c524-4c2b-f62b-21bd43bd6eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9193181818181818\n"
          ]
        }
      ],
      "source": [
        "test_accuracy = evaluate(best_model, test_loader, threshold = 0.5, device = \"cuda\")\n",
        "print(test_accuracy)"
      ],
      "id": "e6a418dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9107414"
      },
      "source": [
        "As you can see we get around ~93% accuracy on the test data! Compare it with ~80% accuracy that we had been getting with the Bag of Words models in previous assignments. This shows how powerful these pre-trained contextual representations can be in solving such NLP tasks."
      ],
      "id": "f9107414"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82f7ea51"
      },
      "source": [
        "### Task 2.3: Making Predictions from scratch (1 Mark)\n",
        "\n",
        "Similar to assignment 1, implement the function `predict_text` that takes as input the sentence/document to be classified and runs it through the BERT classifier model to obtain the prediction."
      ],
      "id": "82f7ea51"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "639651fe"
      },
      "outputs": [],
      "source": [
        "def predict_text(text, model, tokenizer, threshold = 0.5,device = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Predicts the sentiment label for a piece of text using the BERT classifier model\n",
        "    \n",
        "    Inputs:\n",
        "        - text (str): The sentence/document whose sentiment is to be predicted\n",
        "        - model (BertClassifierModel): Fine-tuned BERT based classifer model\n",
        "        - tokenizer (BertTokenizer): Pre-trained BERT tokenizer\n",
        "        - threshold (float): Probability Threshold above which we consider label as 1 and 0 below\n",
        "    Returns:\n",
        "        - pred_label (float): Predicted sentiment of the document\n",
        "    \"\"\"\n",
        "    \n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    pred_label = None\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    tokenizer_output = tokenizer(text, max_length=seq_len, padding=\"max_length\", truncation = True, return_tensors=\"pt\")\n",
        "    input_ids = tokenizer_output[\"input_ids\"]\n",
        "    mask = tokenizer_output[\"attention_mask\"]\n",
        "    #inp=torch.FloatTensor(input_ids)\n",
        "    pred=model(input_ids,mask)\n",
        "    pred = pred.detach().cpu().numpy()\n",
        "    pred_label=convert_probs_to_labels(pred)\n",
        "    #raise NotImplementedError()\n",
        "    \n",
        "    return pred_label"
      ],
      "id": "639651fe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c969000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33016716-1701-4cd5-86f1-b16931f2db42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Cases\n",
            "Sample Test Case 1\n",
            "Sample Text: this movie was great\n",
            "Predicted Label: [1]\n",
            "Expected Label: 1\n",
            "**********************************\n",
            "\n",
            "Sample Test Case 2\n",
            "Sample Text: A terrible film, 2 hours of my life that I will never get back\n",
            "Predicted Label: [0]\n",
            "Expected Label: 0\n",
            "**********************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Running Sample Test Cases\")\n",
        "\n",
        "print(\"Sample Test Case 1\")\n",
        "sample_document = \"this movie was great\"\n",
        "predicted_label = predict_text(sample_document, best_model, bert_tokenizer)\n",
        "expected_label = 1\n",
        "print(f\"Sample Text: {sample_document}\")\n",
        "print(f\"Predicted Label: {predicted_label}\")\n",
        "print(f\"Expected Label: {expected_label}\")\n",
        "\n",
        "assert predicted_label == expected_label\n",
        "\n",
        "print(\"**********************************\\n\")\n",
        "\n",
        "print(\"Sample Test Case 2\")\n",
        "sample_document = \"A terrible film, 2 hours of my life that I will never get back\"\n",
        "predicted_label = predict_text(sample_document, best_model, bert_tokenizer)\n",
        "expected_label = 0\n",
        "print(f\"Sample Text: {sample_document}\")\n",
        "print(f\"Predicted Label: {predicted_label}\")\n",
        "print(f\"Expected Label: {expected_label}\")\n",
        "\n",
        "assert predicted_label == expected_label\n",
        "\n",
        "print(\"**********************************\\n\")\n",
        "\n"
      ],
      "id": "7c969000"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc0734cd"
      },
      "source": [
        "## Task 3: Fine-tuning BERT on Micorsoft Research Paraphrase Corpus (5 Marks)\n",
        "\n",
        "Micorsoft Research Paraphrase Corpus (MRPC) consists of sentence pairs extracted from online news sources and the task is to identify whether the two sentences are paraphrases of each other i.e. if they have the same meaning. Unlike SST-2 this task operates on a pair of sentences instead of a single sentence. However, the way BERT is trained it makes it very easy to handle pair of sentences by just seperating them via a \\[SEP\\] token\n",
        "\n",
        "<img src=\"https://i.ibb.co/Nx8mK1P/bert-sentence-pair.jpg\" alt=\"bert-sentence-pair\" border=\"0\">\n",
        "\n",
        "Hence we just need to modify the custom dataset to do this concatenation operation and rest of the code for models, training and evaluation can essentially stay the same! We load the dataset below:"
      ],
      "id": "dc0734cd"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b5c990ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea357a5-9f93-4839-bbc4-27146302038c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Training Examples: 3668\n",
            "Number of Validation Examples: 408\n",
            "Number of Test Examples: 1725\n"
          ]
        }
      ],
      "source": [
        "def load_mrpc_dataset(split = \"train\"):\n",
        "    filename = os.path.join(mrpc_data_dir, f\"msr_paraphrase_{split}.txt\")\n",
        "    sentence1s = []\n",
        "    sentence2s = []\n",
        "    labels = []\n",
        "    with open(filename) as f:\n",
        "        for i,line in enumerate(f):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            row = line.split(\"\\t\")\n",
        "            sentence1 = row[3]\n",
        "            sentence2 = row[4]\n",
        "            label = row[0]\n",
        "            sentence1s.append(sentence1)\n",
        "            sentence2s.append(sentence2)\n",
        "            labels.append(int(label))\n",
        "    \n",
        "    return pd.DataFrame({\n",
        "        \"sentence1\": sentence1s,\n",
        "        \"sentence2\" : sentence2s,\n",
        "        \"label\" : labels\n",
        "    })\n",
        "\n",
        "\n",
        "mrpc_train_df = load_mrpc_dataset(\"train\")\n",
        "mrpc_train_df, mrpc_val_df = train_test_split(mrpc_train_df, test_size=0.1, random_state=42)\n",
        "mrpc_test_df = load_mrpc_dataset(\"test\")\n",
        "\n",
        "print(f\"Number of Training Examples: {len(mrpc_train_df)}\")\n",
        "print(f\"Number of Validation Examples: {len(mrpc_val_df)}\")\n",
        "print(f\"Number of Test Examples: {len(mrpc_test_df)}\")"
      ],
      "id": "b5c990ee"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "181cbceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ae7c2567-4baf-4ea7-fe81-ee0d521c7ecb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              sentence1  \\\n",
              "1789  No Americans were reported among the casualtie...   \n",
              "393   Microsoft is preparing to alter its Internet E...   \n",
              "2390  \"This fire is going to have a great potential ...   \n",
              "1940  Federal offices were to remain closed for a se...   \n",
              "170   In Canada, the booming dollar will be in focus...   \n",
              "\n",
              "                                              sentence2  label  \n",
              "1789  None of the casualties was Americans, said Cap...      1  \n",
              "393   Microsoft Corp. is preparing changes to its In...      1  \n",
              "2390  \"The fire is going to have great potential to ...      1  \n",
              "1940  The Government shut down in Washington, and fe...      1  \n",
              "170   In Canada, the surging dollar was in focus aga...      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7578473-2de1-40f3-8559-b3e2d7a845bb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1789</th>\n",
              "      <td>No Americans were reported among the casualtie...</td>\n",
              "      <td>None of the casualties was Americans, said Cap...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>Microsoft is preparing to alter its Internet E...</td>\n",
              "      <td>Microsoft Corp. is preparing changes to its In...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2390</th>\n",
              "      <td>\"This fire is going to have a great potential ...</td>\n",
              "      <td>\"The fire is going to have great potential to ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1940</th>\n",
              "      <td>Federal offices were to remain closed for a se...</td>\n",
              "      <td>The Government shut down in Washington, and fe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>In Canada, the booming dollar will be in focus...</td>\n",
              "      <td>In Canada, the surging dollar was in focus aga...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7578473-2de1-40f3-8559-b3e2d7a845bb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a7578473-2de1-40f3-8559-b3e2d7a845bb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a7578473-2de1-40f3-8559-b3e2d7a845bb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "mrpc_train_df.head()"
      ],
      "id": "181cbceb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15e8ab0f"
      },
      "source": [
        "The `\"sentence1\"` and `\"sentence2\"` contain the two sentences respectively, and the `\"label\"` column contains the label where 1 indicates the two sentences are paraphrases and 0 otherwise."
      ],
      "id": "15e8ab0f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c9ddfb8"
      },
      "source": [
        "From here we remove the training wheels and ask you to implement the fine-tuning pipeline for this task yourself. As mentioned before there will be very few changes needed over the functions/classes we have already defined for fine-tuning on SST-2 dataset. We will evaluate based on whether you could fine-tune the model on the MRPC dataset and evaluate it on its test set. You should expect an accuracy of about ~83% on the test set."
      ],
      "id": "4c9ddfb8"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cad68623"
      },
      "outputs": [],
      "source": [
        "# Fine-Tune BERT on MRPC corpus\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MRPCBertDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, sentences, labels, seq_len, bert_variant = \"bert-base-uncased\"):\n",
        "        \"\"\"\n",
        "        Constructor for the `SST2BertDataset` class. Stores the `sentences` and `labels` which can then be used by\n",
        "        other methods. Also initializes the tokenizer\n",
        "        \n",
        "        Inputs:\n",
        "            - sentences (list) : A list of movie reviews\n",
        "            - labels (list): A list of sentiment labels corresponding to each review\n",
        "            - seq_len (int): Length of the sequence to use.\n",
        "                             If number of tokens are lower than `seq_len` add padding otherwise truncate\n",
        "        \"\"\"\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.seq_len = seq_len\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_variant)\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        \n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset i.e. the number of reviews present in the dataset\n",
        "        \"\"\"\n",
        "        length = len(self.labels)\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        \n",
        "        return length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns the training example corresponding to review present at the `idx` position in the dataset\n",
        "        \n",
        "        Inputs:\n",
        "            - idx (int): Index corresponding to the review,label to be returned\n",
        "            \n",
        "        Returns:\n",
        "            - input_ids (torch.tensor): Indices of the tokens in the sentence at `idx` position.\n",
        "                                        Shape of the tensor should be (`seq_len`,)\n",
        "            - mask (torch.tensor): Attention mask indicating which tokens are padded.\n",
        "                                   Shape of the tensor should be (`seq_len`,)\n",
        "            - label (int): Sentiment label for the corresponding sentence\n",
        "        \n",
        "        Hint: To get the output from the tokenizer in the form of torch tensors set return_tensors=\"pt\" when calling self.tokenizer \n",
        "        \"\"\"\n",
        "        tokenizer_output = self.tokenizer(self.sentences[idx], max_length=self.seq_len, padding=\"max_length\", truncation = True, return_tensors=\"pt\")\n",
        "        input_ids = tokenizer_output[\"input_ids\"]\n",
        "        mask = tokenizer_output[\"attention_mask\"]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        \n",
        "        return input_ids.squeeze(0), mask.squeeze(0), label\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()"
      ],
      "id": "cad68623"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ef516c5"
      },
      "outputs": [],
      "source": [
        "seq_len = 128\n",
        "batch_size = 16\n",
        "\n",
        "train_sentences, train_labels = mrpc_train_df[\"sentence1\"].values +\" \"+ mrpc_train_df[\"sentence2\"].values, mrpc_train_df[\"label\"].values\n",
        "val_sentences, val_labels = mrpc_val_df[\"sentence1\"].values+\" \"+ mrpc_val_df[\"sentence2\"].values, mrpc_val_df[\"label\"].values\n",
        "test_sentences, test_labels = mrpc_test_df[\"sentence1\"].values +\" \"+mrpc_test_df[\"sentence2\"].values, mrpc_test_df[\"label\"].values\n",
        "\n",
        "train_dataset = MRPCBertDataset(train_sentences, train_labels, seq_len=seq_len)\n",
        "val_dataset = MRPCBertDataset(val_sentences, val_labels, seq_len=seq_len)\n",
        "test_dataset = MRPCBertDataset(test_sentences, test_labels, seq_len=seq_len)\n",
        "print(train_dataset[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "id": "5ef516c5"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "print(\"Training on 100 data points for sanity check\")\n",
        "sample_sentences = (mrpc_train_df[\"sentence1\"].values +\" \"+ mrpc_train_df[\"sentence2\"].values).tolist()\n",
        "sample_labels = mrpc_train_df[\"label\"].values.tolist()\n",
        "sample_dataset = MRPCBertDataset(sample_sentences, sample_labels, seq_len=32)\n",
        "sample_dataloader = DataLoader(sample_dataset, batch_size=4)\n",
        "\n",
        "model = BertClassifierModel()\n",
        "best_model, best_val_acc = train(model, sample_dataloader, sample_dataloader, num_epochs = 5, device = \"cuda\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc}\")\n",
        "#print(f\"Expected Best Validation Accuracy: {0.99}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WzGLBtODTWa",
        "outputId": "9d8f7a17-b419-46b3-90ad-076cbd33d099"
      },
      "id": "2WzGLBtODTWa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 100 data points for sanity check\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 18.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 completed | Average Training Loss: 0.6394146728515625 | Validation Accuracy: 0.78\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 19.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed | Average Training Loss: 0.5547140920162201 | Validation Accuracy: 0.78\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 18.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed | Average Training Loss: 0.5231435906887054 | Validation Accuracy: 0.78\n",
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 19.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 completed | Average Training Loss: 0.48918630242347716 | Validation Accuracy: 0.78\n",
            "4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 19.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 completed | Average Training Loss: 0.4590074527263641 | Validation Accuracy: 0.81\n",
            "Best Validation Accuracy: 0.81\n",
            "Expected Best Validation Accuracy: 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertClassifierModel()\n",
        "best_model, best_val_acc = train(model, train_loader, val_loader, num_epochs = 5, device = \"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScPg0p4DfnvS",
        "outputId": "ccdcdffb-ec6f-4bcd-abbf-46ddae463ba3"
      },
      "id": "ScPg0p4DfnvS",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230/230 [01:19<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 completed | Average Training Loss: 0.5563013709109762 | Validation Accuracy: 0.8432065217391305\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230/230 [01:22<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed | Average Training Loss: 0.3494829890682645 | Validation Accuracy: 0.9179347826086957\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230/230 [01:25<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed | Average Training Loss: 0.1963607352550911 | Validation Accuracy: 0.9467391304347826\n",
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230/230 [01:26<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 completed | Average Training Loss: 0.1284814048760935 | Validation Accuracy: 0.9888586956521739\n",
            "4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230/230 [01:26<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 completed | Average Training Loss: 0.07352128946627287 | Validation Accuracy: 0.9896739130434783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = evaluate(best_model, test_loader, threshold = 0.5, device = \"cuda\")\n",
        "print(test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMLE0eN8hbm_",
        "outputId": "7c3e8670-2f16-493b-f58b-529d2cc5003d"
      },
      "id": "oMLE0eN8hbm_",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8227831196581197\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "82f7ea51"
      ],
      "name": "Assignment3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c177ee743654a7f856594eb44f1e252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14fbe477deb8405cbc0077efe9ffb2bd",
              "IPY_MODEL_9faab1f4a97d4491898b85677ca88c1f",
              "IPY_MODEL_32c2d90041374ab39ac5d1c4cdaef842"
            ],
            "layout": "IPY_MODEL_b0ff83fea7464fe893b09b5a47a59ae4"
          }
        },
        "14fbe477deb8405cbc0077efe9ffb2bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b07c28d9769044248a75b274fab48b9b",
            "placeholder": "​",
            "style": "IPY_MODEL_026ea7d1bf0f4c8ba77178192fa7943d",
            "value": "Downloading: 100%"
          }
        },
        "9faab1f4a97d4491898b85677ca88c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb3dd0f3f06d497daf36edc485d638ff",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17ad1cb339d3406f97c48659e521d3e9",
            "value": 231508
          }
        },
        "32c2d90041374ab39ac5d1c4cdaef842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb43cdd30f3e40ae8228bce7e65dbc35",
            "placeholder": "​",
            "style": "IPY_MODEL_dde9a52b53ce4a4babb8d08ca1b191d5",
            "value": " 226k/226k [00:00&lt;00:00, 5.29MB/s]"
          }
        },
        "b0ff83fea7464fe893b09b5a47a59ae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b07c28d9769044248a75b274fab48b9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "026ea7d1bf0f4c8ba77178192fa7943d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb3dd0f3f06d497daf36edc485d638ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17ad1cb339d3406f97c48659e521d3e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb43cdd30f3e40ae8228bce7e65dbc35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dde9a52b53ce4a4babb8d08ca1b191d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c87f7c27b0a643b99ca14f592cca36a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a4f2356e8c847aabd777c12aaf7c898",
              "IPY_MODEL_046905ed284549828924f10ddc554afa",
              "IPY_MODEL_71092919c40f4831a70993d06b0ed8e9"
            ],
            "layout": "IPY_MODEL_73c6c3a44d464e96b88048f979c1f8f3"
          }
        },
        "3a4f2356e8c847aabd777c12aaf7c898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cee0b0e54a104eeab139022dc1af1ad8",
            "placeholder": "​",
            "style": "IPY_MODEL_85b6ddda93254dd1af18f8a4f937b753",
            "value": "Downloading: 100%"
          }
        },
        "046905ed284549828924f10ddc554afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ba73991423947c3a24256b03aa21696",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa2e9185e7954413a9063c72944e744b",
            "value": 28
          }
        },
        "71092919c40f4831a70993d06b0ed8e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dea7ea618c74a4cbf591eb0c13688a5",
            "placeholder": "​",
            "style": "IPY_MODEL_89f08eb270eb44658e035c1c87961a54",
            "value": " 28.0/28.0 [00:00&lt;00:00, 876B/s]"
          }
        },
        "73c6c3a44d464e96b88048f979c1f8f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cee0b0e54a104eeab139022dc1af1ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85b6ddda93254dd1af18f8a4f937b753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ba73991423947c3a24256b03aa21696": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa2e9185e7954413a9063c72944e744b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4dea7ea618c74a4cbf591eb0c13688a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89f08eb270eb44658e035c1c87961a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0536691395494e01a52e4714bcb61dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0a3e59425744c45ae87bde298bfb7fa",
              "IPY_MODEL_e4b9775c37ab401cbcb24dbc44f2ff48",
              "IPY_MODEL_07c9da16915b4727ab981ecf076941b2"
            ],
            "layout": "IPY_MODEL_5bd4089f2ca8488a86f9b54a3359992d"
          }
        },
        "d0a3e59425744c45ae87bde298bfb7fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aecbe9da0699413e98fac8a668751d74",
            "placeholder": "​",
            "style": "IPY_MODEL_d1b2b311866a40e0ab57634fc85301f2",
            "value": "Downloading: 100%"
          }
        },
        "e4b9775c37ab401cbcb24dbc44f2ff48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23309253219648bb8ff941fba0db3f8e",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97f13130431c46a2840d15ae3be041c9",
            "value": 570
          }
        },
        "07c9da16915b4727ab981ecf076941b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6637a2d35ed84a6e8217ff4c5c574f0f",
            "placeholder": "​",
            "style": "IPY_MODEL_2e445a0c08214f2f91b99bfa19d0f8b7",
            "value": " 570/570 [00:00&lt;00:00, 16.4kB/s]"
          }
        },
        "5bd4089f2ca8488a86f9b54a3359992d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aecbe9da0699413e98fac8a668751d74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b2b311866a40e0ab57634fc85301f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23309253219648bb8ff941fba0db3f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97f13130431c46a2840d15ae3be041c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6637a2d35ed84a6e8217ff4c5c574f0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e445a0c08214f2f91b99bfa19d0f8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "930269b3fa1f468791afcf0f03b2d383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95e7cc5fb8d241fbb6d74f78c7cfd77c",
              "IPY_MODEL_66b13032a2764b3e9ac225677c757610",
              "IPY_MODEL_44f3810657174c46a7b330a867ffecf1"
            ],
            "layout": "IPY_MODEL_5fc04757b46f4f41abb5553b6149fa5a"
          }
        },
        "95e7cc5fb8d241fbb6d74f78c7cfd77c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37c647f3384c435892b9bda4cdbcdf24",
            "placeholder": "​",
            "style": "IPY_MODEL_7b21d599b26d41239ae776badabc5dbd",
            "value": "Downloading: 100%"
          }
        },
        "66b13032a2764b3e9ac225677c757610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_488cfe0abc7f49188acc43a4f9c616e1",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a76ac3e54f8b4de3bf1e287b51a79d85",
            "value": 440473133
          }
        },
        "44f3810657174c46a7b330a867ffecf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac8bdbf63eef4296be62f8ff43df1082",
            "placeholder": "​",
            "style": "IPY_MODEL_cebddec4b7944291b34eeef3fa2b1bc1",
            "value": " 420M/420M [00:08&lt;00:00, 59.4MB/s]"
          }
        },
        "5fc04757b46f4f41abb5553b6149fa5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37c647f3384c435892b9bda4cdbcdf24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b21d599b26d41239ae776badabc5dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "488cfe0abc7f49188acc43a4f9c616e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a76ac3e54f8b4de3bf1e287b51a79d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac8bdbf63eef4296be62f8ff43df1082": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cebddec4b7944291b34eeef3fa2b1bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}